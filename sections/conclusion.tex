% !TEX root = main.tex

\section{Conclusion \& Future Work}

This paper proposes a new direction for accessibility evaluation: the use of autonomous, multi-modal \ac{AI} agents that simulate visual impairments and interact with web content through perception, not code. By combining visual filters, screen reader output, and task-based prompting, this approach aims to approximate the experiences of users with diverse visual disabilities and to surface accessibility issues that may be missed by static tools.

The potential of this method lies in its ability to reveal failures that only emerge under perceptual constraints, and to provide richer, multi-modal insights into web accessibility. At the same time, we recognize the methodological challenges of using simulated agents as proxies for real users, and stress the importance of human validation.

Future work will involve comparing this approach with existing methods, such as seeking consultation services from accessibility experts, comparison with real user evaluations and static methods. Together with a detailed analysis of system performance and cost-effectiveness, weighing the benefits of automation against the increased computational demands. Ultimately, we hope this research will lay the groundwork for more dynamic, user-centered accessibility evaluation methods and inspire further research in this area.


% * The paper could be improved by introducing further analysis and discussion on the system overhead, as deploying compute-intensive LLM-based AI agents requires much more resources than conventional static approaches.

% * future work: the impact and cost-benefit of using this instead of static, carlos example, cost of hiring someone v.s. using this solution. e.g. inference
