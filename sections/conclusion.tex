% !TEX root = main.tex

\section{Conclusion \& Future Work}

This paper proposes a new direction for accessibility evaluation: the use of autonomous, multi-modal \ac{AI} agents that simulate visual impairments and interact with web content through perception, not code. By combining visual filters, screen reader output, and task-based prompting, this approach aims to approximate the experiences of users with diverse visual disabilities and to surface accessibility issues that may be missed by static tools.

The potential of this method lies in its ability to reveal failures that only emerge under perceptual constraints, and to provide richer, multi-modal insights into web accessibility. At the same time, we recognize the methodological challenges of using simulated agents as proxies for real users, and stress the importance of human validation.

As next steps, we plan to design and implement a prototype system that integrates visual impairment simulation, screen reader emulation, and agent-based control. We will define representative tasks and benchmark scenarios to explore how different impairments and input modes affect accessibility outcomes. Future work will also involve comparing this approach with existing static tools, and seeking feedback from accessibility experts and users. Ultimately, we hope this research will lay the groundwork for more dynamic, user-centered accessibility evaluation methods and inspire further investigation in this area.