% !TEX root = main.tex

\section{Related Work}
\subsection{Web accessibility compliance}

Conventional web accessibility assessments primarily rely on static checkers that verify compliance with \ac{WCAG} criteria by inspecting HTML and CSS structures. Tools including WAVE by WebAIM\cite{webaim_wave_2025} and IBM's NPM accessibility-checker\cite{ibm_accessibility_checker_2025} exemplify this approach. However, several studies have revealed significant limitations in these tools, noting they often lack semantic awareness and fail to consider user perspectives, leading to incomplete or misleading results \cite{ara2024inclusive}. 

For instance, Todorov et al. evaluated Bulgarian museum websites uncovered widespread accessibility failures despite technical correctness \cite{todorov2022accessibility}. Similarly, Inal et al. conducted a study of Norwegian municipal websites and found that although legislation mandated \ac{WCAG} compliance, issues such as low-quality alternative text persisted.
Chiou et al. further observed that many common web accessibility issues arise in responsive sites during resizing, highlighting the importance of evaluating accessibility across different device viewports\cite{chiou2024automatically}.

Complimentarily, \ac{WCAG} conformance varies by level, from A (lower) to AAA (higher), meaning that a website labeled as “compliant” may still fall short of more stringent and specific accessibility standards. These findings underscore the need for real-time evaluations that reflect how visually impaired users interact with web interfaces. 
While automated tools are useful for early feedback, they are unable to identify all critical accessibility, functionality, and usability issues affecting this population \cite{todorov2022accessibility}.

\subsection{Automated accessibility Testing}

Recent studies introduce hybrid frameworks that combine guidelines with heuristic, automated, or AI-driven methods to improve accessibility evaluation. Watanabe et al.~\cite{watanabe2024accessibility} show that machine learning can detect ARIA landmarks in web apps, revealing how classifiers may infer structure when key accessibility tags are missing. Similarly, models have been used to identify and remediate accessibility issues, helping sites align with accessibility standards. Some approaches analyze source code to suggest fixes~\cite{ramineni2024leveraging, kuszczynski2023comparative}, while others operate on rendered pages using prompt-based methods~\cite{he2025enhancing}. Mehralian et al.~\cite{mehralian2025automated} propose a rule-based system for testing dynamic content in Android apps, and Tafreshipour et al.~\cite{tafreshipour2024ma11y} show that mutation testing can uncover additional errors by exploring app states and applying accessibility rules. These efforts reflect growing recognition of the limits of static, rule-based tools, while still relying on source code analysis, not simulating user behavior.


\subsection{Autonomous AI agents}

The use of autonomous \ac{AI} agents to simulate user interaction with web interfaces has also been explored. Lu et al. \cite{lu2025uxagent} introduce UXAgent; a notable system that uses LLM agents to mimic thousands of diverse user personas in web usability studies. The agents interact with live websites via browser automation, providing qualitative and quantitative feedback that supports iterative UX design. 

Complementing this, a GitHub Copilot extension that proactively embeds accessibility guidance into the coding workflow was unveiled by Mowar et al. \cite{mowar2025codea11y}. Their study shows how \ac{AI} assistants can suggest accessible \ac{UI} code, highlight missing attributes, and prompt manual verification during development.

While not strictly agent-based, Zhong et al. \cite{zhong2025screenaudit} leveraged large language models to identify discrepancies between screen reader outputs, providing a novel approach to detecting accessibility errors.

Another recent advancement in this area is AXNav by Taeb et al. \cite{taeb2024axnav}, a system that interprets mobile accessibility test instructions written in natural language and executes them on remote cloud devices using an LLM-based multiagent planner. This approach demonstrates how LLM-driven agents can automate complex evaluations and provide actionable, context-rich feedback for developers.

Multimodal agents that adaptively present content based on user needs, transforming visual content into speech or simplified visuals for users with auditory or visual processing disorders were also proposed by Rajagopal et al. \cite{rajagopal2023design}. While not focused on web testing, their work shows how agents can model disability-specific interactions across modalities.

\section{Automated Testing} % ? No se si dejar este titulo, y si si seria otra seccion aparte

Dynamic testing can help developers and stakeholders verify and validate that running software is working as expected\cite{vasquez2018continuous}. In this case, automated testing is the enabler for faster, more reliable and overall optimized testing. Automated testing leveraged by machine learning and \ac{AI} are increasingly becoming more popular and sophisticated, therefore using them in this scenario is a big step forward for ensuring accessibility.

Intelligent agents can interact with web content without access to the underlying code, relying instead on the same content the final user is interacting with\cite{lanham2025ai, wang2024survey, lu2025uxagent}. They can also be adapted based on feedback, and can process rich multimodal inputs, namely screenshots and screen reader text, enabling them to reason about both the visual layout and the spoken feedback of the user interface simultaneously. They are also able to use different interaction techniques, like keyboard only, mouse only, etc. 

Finally, LLM agents excel at open-ended reasoning and can provide qualitative insights, alongside quantitative logs. However, they require careful prompting and can be slower or less predictable.
\vspace{-8pt}