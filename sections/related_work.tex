% !TEX root = main.tex

\section{Related Work}
\subsection{Web a11y compliance}

Conventional web \ac{a11y} assessments primarily rely on static checkers that verify compliance with \ac{WCAG} criteria by inspecting HTML and CSS structures. Tools including WAVE by WebAIM\cite{webaim_wave_2025} and IBM's NPM accessibility-checker\cite{ibm_accessibility_checker_2025} exemplify this approach. However, several studies have revealed significant limitations in these tools, noting they often lack semantic awareness and fail to consider user perspectives, leading to incomplete or misleading results \cite{ara2024inclusive}. 

For instance, Todorov et al. evaluated Bulgarian museum websites uncovered widespread \ac{a11y} failures despite technical correctness \cite{todorov2022accessibility}. Similarly, Inal et al. conducted a study of Norwegian municipal websites and found that although legislation mandated \ac{WCAG} compliance, issues such as low-quality alternative text persisted.
Chiou et al. further observed that many common web \ac{a11y} issues arise in responsive sites during resizing, highlighting the importance of evaluating \ac{a11y} across different device viewports\cite{chiou2024automatically}.

Complimentarily, \ac{WCAG} conformance varies by level, from A (lower) to AAA (higher), meaning that a website labeled as “compliant” may still fall short of more stringent and specific \ac{a11y} standards. These findings underscore the need for real-time evaluations that reflect how visually impaired users interact with web interfaces. 
While automated tools are useful for early feedback, they are unable to identify all critical \ac{a11y}, functionality, and usability issues affecting this population \cite{todorov2022accessibility}.

\subsection{Automated a11y Testing}

Recent studies introduce hybrid frameworks that combine guidelines with heuristic, automated, or AI-driven methods to improve a11y evaluation. Watanabe et al.~\cite{watanabe2024accessibility} show that machine learning can detect ARIA landmarks in web apps, revealing how classifiers may infer structure when key \ac{a11y} tags are missing. Similarly, models have been used to identify and remediate \ac{a11y} issues, helping sites align with accessibility standards. Some approaches analyze source code to suggest fixes~\cite{ramineni2024leveraging, kuszczynski2023comparative}, while others operate on rendered pages using prompt-based methods~\cite{he2025enhancing}. Mehralian et al.~\cite{mehralian2025automated} propose a rule-based system for testing dynamic content in Android apps, and Tafreshipour et al.~\cite{tafreshipour2024ma11y} show that mutation testing can uncover additional errors by exploring app states and applying accessibility rules. These efforts reflect growing recognition of the limits of static, rule-based tools, while still relying on source code analysis, not simulating user behavior.


\subsection{Autonomous AI agents}

The use of autonomous \ac{AI} agents to simulate user interaction with web interfaces has also been explored. Lu et al. \cite{lu2025uxagent} introduce UXAgent; a notable system that uses LLM agents to mimic thousands of diverse user personas in web usability studies. The agents interact with live websites via browser automation, providing qualitative and quantitative feedback that supports iterative UX design. 

Complementing this, a GitHub Copilot extension that proactively embeds \ac{a11y} guidance into the coding workflow was unveiled by Mowar et al. \cite{mowar2025codea11y}. Their study shows how \ac{AI} assistants can suggest accessible \ac{UI} code, highlight missing attributes, and prompt manual verification during development.

While not strictly agent-based, Zhong et al. \cite{zhong2025screenaudit} leveraged large language models to identify discrepancies between screen reader outputs, providing a novel approach to detecting accessibility errors.

Another recent advancement in this area is AXNav by Taeb et al. \cite{taeb2024axnav}, a system that interprets mobile \ac{a11y} test instructions written in natural language and executes them on remote cloud devices using an LLM-based multiagent planner. This approach demonstrates how LLM-driven agents can automate complex evaluations and provide actionable, context-rich feedback for developers.

Multimodal agents that adaptively present content based on user needs, transforming visual content into speech or simplified visuals for users with auditory or visual processing disorders were also proposed by Rajagopal et al. \cite{rajagopal2023design}. While not focused on web testing, their work shows how agents can model disability-specific interactions across modalities.

