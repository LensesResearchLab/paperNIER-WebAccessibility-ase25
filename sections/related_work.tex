% !TEX root = main.tex

\section{Related Work}
\subsection{Web accessibility compliance}

Conventional web accessibility assessments primarily rely on static checkers that verify compliance with \ac{WCAG} criteria by inspecting HTML and CSS structures. Tools including WAVE by WebAIM\cite{webaim_wave_2025} and IBM's NPM accessibility-checker\cite{ibm_accessibility_checker_2025} exemplify this approach. However, several studies have revealed significant limitations in these tools, noting they often lack semantic awareness and fail to consider user perspectives, leading to incomplete or misleading results \cite{ara2024inclusive}. 

For instance, Todorov et al. evaluated Bulgarian museum websites uncovered widespread accessibility failures despite technical correctness \cite{todorov2022accessibility}. Similarly, Inal et al. conducted a study of Norwegian municipal websites and found that although legislation mandated \ac{WCAG} compliance, issues such as low-quality alternative text persisted.
Chiou et al. further observed that many common web accessibility issues arise in responsive sites during resizing, highlighting the importance of evaluating accessibility across different device viewports\cite{chiou2024automatically}.

Complimentarily, \ac{WCAG} conformance varies by level, from A (lower) to AAA (higher), meaning that a website labeled as “compliant” may still fall short of more stringent and specific accessibility standards. These findings underscore the need for real-time evaluations that reflect how visually impaired users interact with web interfaces. 
While automated tools are useful for early feedback, they are unable to identify all critical accessibility, functionality, and usability issues affecting this population \cite{todorov2022accessibility}.
\vspace{-8pt}

\subsection{Automated accessibility Testing}

Recent studies introduce hybrid frameworks that combine guidelines with heuristic, automated, or AI-driven methods to improve accessibility evaluation. Watanabe et al.~\cite{watanabe2024accessibility} show that machine learning can detect ARIA landmarks in web apps, revealing how classifiers may infer structure when key accessibility tags are missing. Similarly, models have been used to identify and remediate accessibility issues, helping sites align with accessibility standards. Some approaches analyze source code to suggest fixes~\cite{ramineni2024leveraging, kuszczynski2023comparative}, while others operate on rendered pages using prompt-based methods~\cite{he2025enhancing}. Mehralian et al.~\cite{mehralian2025automated} propose a rule-based system for testing dynamic content in Android apps, and Tafreshipour et al.~\cite{tafreshipour2024ma11y} show that mutation testing can uncover additional errors by exploring app states and applying accessibility rules. 

Dynamic testing, as described by Linares-Vásquez et al.~\cite{vasquez2018continuous}, enables developers and stakeholders to verify and validate that running software behaves as expected. In this context, automated testing, especially when enhanced by machine learning and \ac{AI}~\cite{lanham2025ai, wang2024survey, lu2025uxagent}, is becoming increasingly popular and sophisticated, offering faster, more reliable, and more comprehensive accessibility evaluation. 

\subsection{Autonomous AI agents}

The use of autonomous \ac{AI} agents to simulate user interaction with web interfaces has also been explored. Lu et al. \cite{lu2025uxagent} introduce UXAgent; a notable system that uses LLM agents to mimic thousands of diverse user personas in web usability studies. The agents interact with live websites via browser automation, providing qualitative and quantitative feedback that supports iterative UX design. 

Complementing this, a GitHub Copilot extension that proactively embeds accessibility guidance into the coding workflow was unveiled by Mowar et al. \cite{mowar2025codea11y}. Their study shows how \ac{AI} assistants can suggest accessible \ac{UI} code, highlight missing attributes, and prompt manual verification during development.

While not strictly agent-based, Zhong et al. \cite{zhong2025screenaudit} leveraged large language models to identify discrepancies between screen reader outputs, providing a novel approach to detecting accessibility errors.

Another recent advancement in this area is AXNav by Taeb et al. \cite{taeb2024axnav}, a system that interprets mobile accessibility test instructions written in natural language and executes them on remote cloud devices using an LLM-based multiagent planner. This approach demonstrates how LLM-driven agents can automate complex evaluations and provide actionable, context-rich feedback for developers.

Multimodal agents that adaptively present content based on user needs, transforming visual content into speech or simplified visuals for users with auditory or visual processing disorders were also proposed by Rajagopal et al. \cite{rajagopal2023design}. While not focused on web testing, their work shows how agents can model disability-specific interactions across modalities.

Collectively, these works~\cite{lanham2025ai, wang2024survey, lu2025uxagent} illustrate a shift toward agent-based, multimodal, and LLM-driven approaches that move beyond static code analysis to more closely approximate real user experiences. Such agents can interact with web content without access to the underlying code, adapt based on feedback, and process rich multimodal inputs, enabling them to reason about both the visual layout and the spoken feedback of the user interface. While LLM agents excel at open-ended reasoning and can provide qualitative insights alongside quantitative logs, they require careful prompting and can be slower or less predictable.

\vspace{-4pt}