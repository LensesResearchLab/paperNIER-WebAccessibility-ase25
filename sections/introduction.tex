% !TEX root = main.tex

\section{Introduction}

Web accessibility is central to inclusive software design, yet existing evaluation methods predominantly focus on code conformance, such as \ac{WCAG} rule checks, often overlooking how individuals with disabilities actually navigate and perceive interfaces in practice \cite{ara2024inclusive}. Although these tools are useful for guidance and a first approach, they can be unable to thoroughly assess how these impairments affect real-time usability, task completion, or perception of interface elements. While often ignoring behavioral context; they do not simulate navigation, focus order, or sequential interaction flows that can significantly impact accessibility. Individuals with disabilities have diverse needs and experiences, many of which may not be fully addressed by level-A \ac{WCAG} guidelines alone.

Although the access to information is a human right, the urgency of dynamic and human-centered accessibility evaluation is also supported by data. Approximately 2.2 billion people globally have some sort of visual impairment, including both near and distant vision issues \cite{who2023vision}. In Colombia, among the estimated 2.65 million people living with a disability by the DANE, approximately 57\% report that vision-related activities present the greatest challenges\cite{DANE2022}. In bigger countries like the U.S.A., the C.D.C. reports that, about 5.5\% of adults (nearly 19 million people) have blindness or serious difficulty seeing\cite{cdc2025disabilities}. Furthermore, screen reader users find that the most problematic items to interact with in a webpage are CAPTCHA, interactive elements, ambiguous links or buttons, unexpected screen changes, lack of keyboard support, among others\cite{webaimsurvey2025}. Even more tellingly, these users extract information from data visualizations 61\% less accurately and take 211\% more time compared to sighted users \cite{wobbrock2021assets}.

We explore the possibility of approaching accessibility testing using autonomous \ac{AI} agents capable of interacting with web pages visually while being exposed to simulated visual impairments. These agents are prompted with specific user tasks (e.g., locating a button, submitting a form) and attempt to complete them. The agents can also integrate outputs from assistive technologies, for example, capturing a screen reader's textual narration as an additional input channel. 

This multi-modal input allows the agent to simulate how a user with various levels of visual impairment (such as glaucoma, cataracts, myopia or low vision, among others) interacts with content, opening the door for dynamic automated testing other existing tools might be unable to uncover. For instance, detecting unuseful alt text or mislabeled controls or mismatches between rendered content and screen reader output, thereby uncovering potential issues that visual or static code checks miss. 

This paper outlines our motivation, proposed approach, poses future evaluation research questions, and discusses implementation considerations for such a system.
